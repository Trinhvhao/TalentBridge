from typing import List, Optional, Dict, Any
from fastapi import FastAPI, File, UploadFile, HTTPException, Request, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic_models import (
    DocumentInfo, DeleteFileRequest, MatchInput, MatchResponse, JobDetails, MatchedJob,
    CVInsightsResponse, CVImproveResponse, ImprovementSuggestion,
    JobSearchInput, JobSearchResponse, JobSearchResult,
    ApplyJobInput, ApplicationResponse, ApplicationItem, ApplicationsResponse,
    DocumentPreviewResponse, SuggestQuestionsInput, SuggestQuestionsResponse, QuestionSuggestion
)
from langchain_utils import match_cv
from db_utils import (
    get_db_connection, insert_cv_record, insert_match_log, get_match_history, get_cached_matches,
    get_all_cvs, delete_cv_record, get_filtered_jobs,
    insert_application, get_applications_by_cv, check_application_exists,
    save_cv_insights, get_cv_insights, save_document_preview, get_document_preview
)
from chroma_utils import preload_jobs as preload_jobs_to_chroma, index_cv_extracts, delete_cv_from_chroma
from ai_analysis import (
    analyze_cv_insights, generate_cv_improvements, generate_why_match, generate_question_suggestions
)
import pdfplumber
import google.generativeai as genai
import re
import uuid
import logging
import os
import json
from datetime import datetime
import time
from dateutil.parser import parse

# ==== CONFIG ====
API_KEY = os.getenv("GOOGLE_API_KEY")
if not API_KEY:
    logging.error("GOOGLE_API_KEY not set")
    raise ValueError("GOOGLE_API_KEY not set")
genai.configure(api_key=API_KEY)

# Logging
logging.basicConfig(filename='app.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = FastAPI()

# ===== CORS MIDDLEWARE =====
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Cho ph√©p t·∫•t c·∫£ origins (development)
    allow_credentials=True,
    allow_methods=["*"],  # Cho ph√©p t·∫•t c·∫£ methods (GET, POST, PUT, DELETE, etc.)
    allow_headers=["*"],  # Cho ph√©p t·∫•t c·∫£ headers
)

# === PATH ===
base_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(base_dir)
data_path = os.path.join(project_root, "data", "jobs_processed.jsonl")
db_path = os.path.join(project_root, "db")
os.makedirs(db_path, exist_ok=True)

@app.on_event("startup")
async def startup_event():
    try:
        logging.info("üîÑ Preloading jobs into Chroma and SQLite...")
        preload_jobs_to_chroma(data_path, batch_size=500)
        logging.info("‚úÖ Preloading completed")
    except Exception as e:
        logging.error(f"Error during startup preload: {str(e)}")
        raise

def extract_text_from_pdf(pdf_path: str) -> str:
    """Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ file PDF."""
    if not os.path.exists(pdf_path) or os.path.getsize(pdf_path) == 0:
        raise HTTPException(status_code=400, detail="PDF file is empty or does not exist")
    try:
        text = ""
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                content = page.extract_text()
                if content:
                    text += content + "\n"
        if not text.strip():
            raise HTTPException(status_code=400, detail="No text extracted from PDF")
        return text
    except Exception as e:
        logging.error(f"Error extracting text from PDF {pdf_path}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to extract text from PDF: {str(e)}")

def extract_cv_info(cv_text: str) -> dict:
    """Tr√≠ch xu·∫•t th√¥ng tin CV t·ª´ vƒÉn b·∫£n, tr·∫£ v·ªÅ JSON theo schema."""
    if not cv_text.strip():
        raise HTTPException(status_code=400, detail="CV text is empty")
    prompt = f"""
    Extract key resume information from the following CV text.
    Return JSON with this exact schema:
    {{
      "name": "",
      "email": "",
      "phone": "",
      "career_objective": "",
      "skills": [],
      "education": [
        {{
          "school": "",
          "degree": "",
          "major": "",
          "start_date": "YYYY-MM-DD",
          "end_date": "YYYY-MM-DD"
        }}
      ],
      "experience": [
        {{
          "company": "non-empty string",
          "title": "",
          "start_date": "YYYY-MM-DD or Present",
          "end_date": "YYYY-MM-DD or Present",
          "description": ""
        }}
      ]
    }}

    IMPORTANT RULES:
    - PRESERVE THE ORIGINAL LANGUAGE of all text fields (name, company, title, description, skills, etc.)
    - DO NOT translate Vietnamese to English or vice versa
    - If the CV is in Vietnamese, keep all data in Vietnamese
    - If the CV is in English, keep all data in English
    - Dates must be in YYYY-MM-DD format (e.g., '2022-01-01') or 'Present' for ongoing experiences
    - The 'company' field must be a non-empty string (use 'Unknown' if not provided)

    CV Text:
    \"\"\"{cv_text}\"\"\"\n"""
    try:
        from tenacity import retry, stop_after_attempt, wait_exponential
        @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
        def call_gemini():
            model = genai.GenerativeModel("gemini-2.5-flash")
            return model.generate_content(prompt)
        
        response = call_gemini()
        result = response.text
        cleaned = re.sub(r"```json|```", "", result).strip()
        cv_info = json.loads(cleaned)
        # ƒê·∫£m b·∫£o d·ªØ li·ªáu h·ª£p l·ªá
        for exp in cv_info.get("experience", []):
            exp["company"] = exp.get("company") or "Unknown"
            exp["title"] = exp.get("title") or "Unknown"
            exp["description"] = exp.get("description") or "No description provided"
            exp["start_date"] = normalize_date(exp.get("start_date", ""))
            exp["end_date"] = normalize_date(exp.get("end_date", ""))
        for edu in cv_info.get("education", []):
            edu["school"] = edu.get("school") or "Unknown"
            edu["degree"] = edu.get("degree") or "Unknown"
            edu["major"] = edu.get("major") or "Unknown"
            edu["start_date"] = normalize_date(edu.get("start_date", ""))
            edu["end_date"] = normalize_date(edu.get("end_date", ""))
        logging.info(f"Extracted CV info: {json.dumps(cv_info, ensure_ascii=False)[:500]}...")
        return cv_info
    except json.JSONDecodeError as e:
        logging.error(f"Error parsing CV info JSON: {str(e)} - Response: {result[:100]}...")
        raise HTTPException(status_code=500, detail="Failed to parse CV information")
    except Exception as e:
        logging.error(f"Error extracting CV info: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to extract CV information")

def parse_cv_input_string(cv_input: str) -> dict:
    """Parse chu·ªói cv_input th√†nh dictionary."""
    try:
        # N·∫øu cv_input l√† JSON string, parse tr·ª±c ti·∫øp
        if cv_input.strip().startswith('{'):
            return json.loads(cv_input)

        # N·∫øu cv_input l√† chu·ªói text, parse th·ªß c√¥ng
        cv_info = {
            "skills": [],
            "career_objective": "",
            "experience": [],
            "education": [],
            "name": "",
            "email": "",
            "phone": ""
        }

        # Gi·∫£ ƒë·ªãnh format: "Skills: ...; Aspirations: ...; Experience: ...; Education: ..."
        sections = re.split(r'(Skills|Aspirations|Experience|Education|Name|Email|Phone):', cv_input, flags=re.IGNORECASE)
        for i in range(1, len(sections), 2):
            key = sections[i].lower()
            value = sections[i + 1].strip()
            if key == "skills":
                cv_info["skills"] = [s.strip() for s in value.split(',') if s.strip()]
            elif key == "aspirations":
                cv_info["career_objective"] = value
            elif key == "name":
                cv_info["name"] = value
            elif key == "email":
                cv_info["email"] = value
            elif key == "phone":
                cv_info["phone"] = value
            elif key == "experience":
                exp_entries = value.split('\n')
                for entry in exp_entries:
                    if entry.strip():
                        exp = {"company": "Unknown", "title": "Unknown", "start_date": "", "end_date": "", "description": ""}
                        fields = re.split(r';|,', entry)
                        for field in fields:
                            if ':' in field:
                                k, v = field.split(':', 1)
                                k = k.strip().lower()
                                v = v.strip()
                                if k in ["company", "title", "description"]:
                                    exp[k] = v
                                elif k in ["start_date", "end_date"]:
                                    exp[k] = normalize_date(v)
                        cv_info["experience"].append(exp)
            elif key == "education":
                edu_entries = value.split('\n')
                for entry in edu_entries:
                    if entry.strip():
                        edu = {"school": "Unknown", "degree": "Unknown", "major": "Unknown", "start_date": "", "end_date": ""}
                        fields = re.split(r';|,', entry)
                        for field in fields:
                            if ':' in field:
                                k, v = field.split(':', 1)
                                k = k.strip().lower()
                                v = v.strip()
                                if k in ["school", "degree", "major"]:
                                    edu[k] = v
                                elif k in ["start_date", "end_date"]:
                                    edu[k] = normalize_date(v)
                        cv_info["education"].append(edu)
        return cv_info
    except json.JSONDecodeError:
        logging.error(f"Invalid JSON in cv_input string: {cv_input[:100]}...")
        raise HTTPException(status_code=400, detail="Invalid JSON format in cv_input")
    except Exception as e:
        logging.error(f"Error parsing cv_input string: {str(e)}")
        raise HTTPException(status_code=400, detail=f"Failed to parse cv_input string: {str(e)}")


def normalize_deadline(val: str) -> str:
    """ƒê∆∞a deadline b·∫•t k·ª≥ (vd 09/10/2025, 2025/10/09, 09-10-2025, '04/10/2025', ...) v·ªÅ YYYY-MM-DD.
    Kh√¥ng parse ƒë∆∞·ª£c th√¨ tr·∫£ v·ªÅ chu·ªói r·ªóng."""
    if not val:
        return ""
    s = str(val).strip()
    if not s or s.lower() in {"n/a", "none", "null", "kh√¥ng x√°c ƒë·ªãnh"}:
        return ""
    # Th·ª≠ parse "dayfirst" ƒë·ªÉ nh·∫≠n d·∫°ng ƒë·ªãnh d·∫°ng Vi·ªát Nam dd/mm/yyyy
    try:
        dt = _dt_parse(s, dayfirst=True, fuzzy=True)
        return dt.strftime("%Y-%m-%d")
    except Exception:
        pass
    # Th·ª≠ v√†i regex ph·ªï bi·∫øn n·∫øu c·∫ßn
    import re
    m = re.match(r"^(\d{2})[/-](\d{2})[/-](\d{4})$", s)
    if m:
        d, mth, y = m.groups()
        try:
            dt = datetime(int(y), int(mth), int(d))
            return dt.strftime("%Y-%m-%d")
        except Exception:
            return ""
    return ""

def get_job_details(job_ids: List[int]) -> List[JobDetails]:
    """L·∫•y chi ti·∫øt c√¥ng vi·ªác t·ª´ job_store d·ª±a tr√™n job_id (chu·∫©n ho√° deadline -> YYYY-MM-DD)."""
    if not job_ids or not all(isinstance(job_id, int) for job_id in job_ids):
        raise HTTPException(status_code=400, detail="job_ids must be a non-empty list of integers")
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            batch_size = 100
            jobs: List[JobDetails] = []
            for i in range(0, len(job_ids), batch_size):
                batch_ids = job_ids[i:i + batch_size]
                placeholders = ",".join(["?"] * len(batch_ids))
                cursor.execute(f"SELECT * FROM job_store WHERE id IN ({placeholders})", batch_ids)
                rows = cursor.fetchall()
                for row in rows:
                    rd = dict(row)

                    # üîß Chu·∫©n ho√° deadline v·ªÅ YYYY-MM-DD ƒë·ªÉ h·ª£p schema Pydantic
                    if "deadline" in rd:
                        rd["deadline"] = normalize_deadline(rd["deadline"])

                    # (T√πy ch·ªçn an to√†n) N·∫øu skills l∆∞u d·∫°ng JSON string, c√≥ th·ªÉ parse:
                    # if isinstance(rd.get("skills"), str) and rd["skills"].strip().startswith("["):
                    #     try:
                    #         rd["skills"] = json.loads(rd["skills"])
                    #     except Exception:
                    #         pass

                    jobs.append(JobDetails(**rd))
            return jobs
    except Exception as e:
        logging.error(f"Error fetching job details for IDs {job_ids}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch job details: {str(e)}")


def normalize_date(date_str: str) -> str:
    """Chu·∫©n h√≥a ƒë·ªãnh d·∫°ng ng√†y th√†nh YYYY-MM-DD ho·∫∑c gi·ªØ 'Present'."""
    if not date_str or date_str.lower() == "present":
        return "Present"
    try:
        parsed = parse(date_str, fuzzy=True)
        return parsed.strftime("%Y-%m-%d")
    except (ValueError, TypeError):
        logging.warning(f"Invalid date format: {date_str}, defaulting to empty")
        return ""

@app.get("/")
async def root():
    return {"message": "CV Matching API is running!"}

@app.post("/upload-cv")
async def upload_cv(file: UploadFile = File(...)):
    """T·∫£i l√™n CV PDF, tr√≠ch xu·∫•t th√¥ng tin, l∆∞u v√†o cv_store v√† Chroma."""
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(status_code=400, detail="Only PDF files are supported")
    max_size = 10 * 1024 * 1024  # 10MB
    if file.size > max_size:
        raise HTTPException(status_code=400, detail="File size exceeds 10MB")
    temp_file_path = f"temp_{file.filename}"
    file_data = None
    try:
        # Read file data
        file_data = await file.read()

        # Save to temp file for processing
        with open(temp_file_path, "wb") as buffer:
            buffer.write(file_data)

        cv_text = extract_text_from_pdf(temp_file_path)
        cv_info = extract_cv_info(cv_text)
        skills = cv_info.get("skills", [])
        aspirations = cv_info.get("career_objective", "")
        education = cv_info.get("education", [])
        experience = cv_info.get("experience", [])
        if not skills and not aspirations:
            raise HTTPException(status_code=400, detail="No skills or career objective found")
        # T·∫°o t√≥m t·∫Øt experience ƒë·ªÉ index
        experience_summary = "\n".join([
            f"{exp.get('title', 'Unknown')} at {exp.get('company', 'Unknown')} ({exp.get('start_date', '')}-{exp.get('end_date', '')}): {exp.get('description', '')}"
            for exp in experience
        ]) if experience else "No experience provided"

        # Insert CV record with file_data
        cv_id = insert_cv_record(file.filename, cv_info, file_data)
        if not cv_id:
            raise HTTPException(status_code=500, detail="Failed to generate cv_id from database")
        try:
            await index_cv_extracts(skills, aspirations, experience_summary, education, cv_id)
        except Exception as e:
            delete_cv_record(cv_id)
            raise HTTPException(status_code=500, detail=f"Failed to index CV to Chroma: {str(e)}")
        logging.info(f"Uploaded and indexed CV {cv_id}: {file.filename}")
        return {
            "message": f"CV {file.filename} uploaded and indexed",
            "cv_id": cv_id,
            "cv_info": cv_info
        }
    except Exception as e:
        logging.error(f"Error uploading CV {file.filename}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to upload CV: {str(e)}")
    finally:
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)

@app.post("/match", response_model=MatchResponse)
async def match_cv_endpoint(input: MatchInput, request: Request):
    """Kh·ªõp CV v·ªõi c√¥ng vi·ªác, s·ª≠ d·ª•ng l·ªçc tr∆∞·ªõc v√† post-processing."""
    start_time = time.time()

    valid_keys = {"job_type", "work_location", "experience", "education", "skills", "deadline_after"}
    cleaned_filters = {k: v for k, v in input.filters.items() if k in valid_keys}
    if cleaned_filters != input.filters:
        logging.warning(f"B·ªô l·ªçc kh√¥ng h·ª£p l·ªá ƒë∆∞·ª£c b·ªè qua: {input.filters}. S·ª≠ d·ª•ng: {cleaned_filters}")

    session_id = input.session_id or str(uuid.uuid4())
    model_name = input.model.value
    cv_id = None

    try:
        # L·∫•y CV
        cv_start = time.time()
        if input.cv_id:
            cv_id = input.cv_id
            if not isinstance(cv_id, int):
                raise HTTPException(status_code=400, detail="cv_id must be an integer")
            with get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT id, filename, cv_info_json FROM cv_store WHERE id = ?", (input.cv_id,))
                cv = cursor.fetchone()
                if not cv:
                    raise HTTPException(status_code=404, detail=f"CV v·ªõi ID {input.cv_id} kh√¥ng t√¨m th·∫•y")
                cv_info = json.loads(cv["cv_info_json"])
        elif input.cv_input:
            cv_id = input.cv_id or str(uuid.uuid4())
            cv_info = parse_cv_input_string(input.cv_input)
            cv_id = insert_cv_record("manual_input", cv_info)
            await index_cv_extracts(cv_info["skills"], cv_info["career_objective"], cv_info["education"], cv_id)
        else:
            cvs = get_all_cvs()
            if not cvs:
                raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y CV n√†o")
            cv = cvs[0]
            cv_id = cv["id"]
            cv_info = json.loads(cv["cv_info_json"])
        logging.info(f"‚úÖ L·∫•y CV {cv_id} th√†nh c√¥ng ({time.time() - cv_start:.2f}s)")

        # Chu·∫©n h√≥a education v√† experience
        input_start = time.time()
        skills = cv_info.get("skills", [])
        aspirations = cv_info.get("career_objective", "")
        education = cv_info.get("education", [])
        experience = cv_info.get("experience", [])

        for edu in education:
            try:
                edu["start_date"] = normalize_date(edu.get("start_date", ""))
                edu["end_date"] = normalize_date(edu.get("end_date", ""))
                edu["school"] = edu.get("school") or "Unknown"
                edu["degree"] = edu.get("degree") or "Unknown"
                edu["major"] = edu.get("major") or "Unknown"
            except Exception as e:
                logging.warning(f"Error normalizing education: {str(e)}")
                edu["start_date"] = ""
                edu["end_date"] = ""
                edu["school"] = "Unknown"
                edu["degree"] = "Unknown"
                edu["major"] = "Unknown"

        for exp in experience:
            try:
                exp["start_date"] = normalize_date(exp.get("start_date", ""))
                exp["end_date"] = normalize_date(exp.get("end_date", ""))
                exp["company"] = exp.get("company") or "Unknown"
                exp["title"] = exp.get("title") or "Unknown"
                exp["description"] = exp.get("description") or "No description provided"
            except Exception as e:
                logging.warning(f"Error normalizing experience: {str(e)}")
                exp["start_date"] = ""
                exp["end_date"] = ""
                exp["company"] = "Unknown"
                exp["title"] = "Unknown"
                exp["description"] = "No description provided"



        experience_summary = "\n".join([
            f"Project: {exp['title']} - {exp['description'][:200] + '...' if len(exp['description']) > 200 else exp['description']}"
            for exp in experience
        ]) if experience else "No experience provided"
        education_summary = "\n".join([
            f"Degree: {edu['degree']} at {edu['school']} ({edu['start_date']}-{edu['end_date']})"
            for edu in education
        ]) if education else "No education provided"
        aspirations_summary = aspirations if aspirations else "No career objective provided"

        cv_input = {
            "skills": skills,
            "aspirations": aspirations_summary,
            "experience": experience_summary,
            "education": education_summary,
            "cv_id": cv_id
        }

        # L·∫•y filtered_job_ids
        filtered_job_ids = get_filtered_jobs(cleaned_filters)
        suggestions = []
        if filtered_job_ids is None:
            suggestions = [{"skill_or_experience": "N/A", "suggestion": "No filters applied or no jobs matched, showing best matches from all jobs."}]
        else:
            logging.info(f"‚úÖ L·ªçc ƒë∆∞·ª£c {len(filtered_job_ids)} jobs")

        # Ki·ªÉm tra cache tr∆∞·ªõc
        cached_jobs = get_cached_matches(cv_id)

        if cached_jobs and not cleaned_filters:
            # C√≥ cache v√† kh√¥ng c√≥ filters ‚Üí D√πng cache
            logging.info(f"üöÄ S·ª≠ d·ª•ng cached matches cho CV {cv_id} (skip RAG)")
            matched_jobs_all = cached_jobs
            suggestions = []
        else:
            # Ch·∫°y RAG v·ªõi match_cv
            try:
                invoke_start = time.time()
                result = await match_cv(cv_input, filtered_job_ids, session_id)
                logging.info(f"‚úÖ Match CV ho√†n t·∫•t ({time.time() - invoke_start:.2f}s)")

                if not result or not isinstance(result, dict):
                    logging.error("match_cv tr·∫£ v·ªÅ k·∫øt qu·∫£ r·ªóng")
                    return MatchResponse(
                        name=cv_info.get("name", ""),
                        email=cv_info.get("email", ""),
                        phone=cv_info.get("phone", ""),
                        cv_skills=skills,
                        career_objective=aspirations,
                        education=education,
                        experience=experience,
                        matched_jobs=[],
                        suggestions=[{"skill_or_experience": "N/A", "suggestion": "Failed to match jobs"}],
                        session_id=session_id,
                        model=input.model
                    )

                answer = result

                # L·∫•y T·∫§T C·∫¢ matched_jobs (c√≥ th·ªÉ l√™n ƒë·∫øn 20)
                matched_jobs_all = answer.get("matched_jobs", [])
                suggestions = answer.get("suggestions", []) or suggestions

                logging.info(f"üìä Nh·∫≠n ƒë∆∞·ª£c {len(matched_jobs_all)} jobs t·ª´ RAG")
            except Exception as e:
                logging.error(f"‚ùå L·ªói match_cv: {e}")
                raise

        # Validate matched_jobs_all
        if not isinstance(matched_jobs_all, list):
            logging.error(f"matched_jobs kh√¥ng ph·∫£i danh s√°ch: {matched_jobs_all}")
            matched_jobs_all = []

        for job in matched_jobs_all[:]:
            if not isinstance(job, dict) or "job_id" not in job or "match_score" not in job:
                logging.warning(f"Job thi·∫øu job_id ho·∫∑c match_score: {job}. Skipping.")
                matched_jobs_all.remove(job)
                continue
            for field in ["matched_skills", "matched_aspirations", "matched_experience", "matched_education", "skills"]:
                if not isinstance(job.get(field, []), list):
                    logging.error(f"Tr∆∞·ªùng {field} kh√¥ng ph·∫£i danh s√°ch trong job: {job}")
                    job[field] = []

        if not isinstance(suggestions, list):
            logging.error(f"suggestions kh√¥ng ph·∫£i danh s√°ch: {suggestions}")
            suggestions = []

        # Post-processing: chu·∫©n h√≥a job_id -> int, enrich t·ª´ DB
        import re
        def _to_int_job_id(x):
            """Nh·∫≠n int, '716', 'job_716'... -> int; invalid -> None"""
            if isinstance(x, int):
                return x
            if isinstance(x, str):
                m = re.search(r"\d+", x.strip())
                if m:
                    try:
                        return int(m.group())
                    except Exception:
                        return None
            return None

        # 1) Chu·∫©n h√≥a danh s√°ch job_id cho T·∫§T C·∫¢ jobs
        job_ids: List[int] = []
        for job in matched_jobs_all:
            jid_int = _to_int_job_id(job.get("job_id"))
            if jid_int is None:
                logging.warning(f"Invalid job_id skipped: {job.get('job_id')}")
                continue
            job_ids.append(jid_int)

        # 2) L·∫•y chi ti·∫øt t·ª´ DB
        if not job_ids:
            job_details = []
        else:
            job_details = get_job_details(job_ids)

        # 3) Map chi ti·∫øt theo INT key (quan tr·ªçng)
        job_details_dict = {int(job.job_id): job for job in job_details if getattr(job, "job_id", None) is not None}

        # 4) Enrich T·∫§T C·∫¢ jobs (l√™n ƒë·∫øn 20 jobs)
        enriched_all_jobs = []
        for job in matched_jobs_all:
            jid_int = _to_int_job_id(job.get("job_id"))
            if jid_int is None:
                continue

            detail = job_details_dict.get(jid_int)
            if not detail:
                logging.warning(f"Job ID {job.get('job_id')} not found in job_details")
                continue

            # match_score c√≥ th·ªÉ l√† 0..1 ho·∫∑c ph·∫ßn trƒÉm >1 -> ƒë∆∞a v·ªÅ 0..1
            ms = job.get("match_score", 0.0)
            try:
                ms = float(ms)
                if ms > 1.0:          # v√≠ d·ª• 62 -> 0.62
                    ms = ms / 100.0
                ms = max(0.0, min(1.0, ms))
            except Exception:
                ms = 0.0

            enriched_all_jobs.append(
                MatchedJob(
                    job_id=str(jid_int),
                    job_title=job.get("job_title") or getattr(detail, "job_title", ""),
                    job_url=(job.get("job_url") or getattr(detail, "job_url", "")),
                    match_score=ms,
                    matched_skills=job.get("matched_skills") or [],
                    matched_aspirations=job.get("matched_aspirations") or [],
                    matched_experience=job.get("matched_experience") or [],
                    matched_education=job.get("matched_education") or [],
                    work_location=getattr(detail, "work_location", None),
                    salary=getattr(detail, "salary", None),
                    deadline=getattr(detail, "deadline", None),
                    benefits=getattr(detail, "benefits", None),
                    job_type=getattr(detail, "work_type", None),
                    experience_required=getattr(detail, "experience", None),
                    education_required=getattr(detail, "education", None),
                    company_name=getattr(detail, "name", None),
                    skills=getattr(detail, "skills", None),
                    why_match=job.get("why_match", None),  # AI-generated reason
                    job_description=getattr(detail, "job_description", None),
                )
            )

        # 5) L∆∞u T·∫§T C·∫¢ jobs v√†o cache (20 jobs)
        safe_all_jobs = [
            {
                "job_id": job.job_id,
                "job_title": job.job_title,
                "job_url": job.job_url,
                "match_score": job.match_score,
                "matched_skills": job.matched_skills,
                "matched_aspirations": job.matched_aspirations,
                "matched_experience": job.matched_experience,
                "matched_education": job.matched_education,
                "work_location": job.work_location,
                "salary": job.salary,
                "deadline": job.deadline,
                "benefits": job.benefits,
                "job_type": job.job_type,
                "experience_required": job.experience_required,
                "education_required": job.education_required,
                "company_name": job.company_name,
                "skills": job.skills,
                "why_match": job.why_match,
                "job_description": job.job_description
            }
            for job in enriched_all_jobs
        ]

        # L∆∞u cache (20 jobs)
        if not cached_jobs:  # Ch·ªâ l∆∞u n·∫øu kh√¥ng d√πng cache
            insert_match_log(session_id, cv_id, safe_all_jobs)
            logging.info(f"üíæ ƒê√£ cache {len(safe_all_jobs)} jobs cho CV {cv_id}")

        # 6) Tr·∫£ v·ªÅ TOP 5 jobs
        top_5_jobs = enriched_all_jobs[:5]

        logging.info(f"‚úÖ Ho√†n t·∫•t! CV {cv_id} matched {len(enriched_all_jobs)} jobs, tr·∫£ v·ªÅ top {len(top_5_jobs)} | T·ªïng th·ªùi gian: {time.time() - start_time:.2f}s")

        return MatchResponse(
            name=cv_info.get("name", ""),
            email=cv_info.get("email", ""),
            phone=cv_info.get("phone", ""),
            cv_skills=skills,
            career_objective=aspirations,
            education=education,
            experience=experience,
            matched_jobs=top_5_jobs,  # Ch·ªâ tr·∫£ v·ªÅ top 5
            suggestions=suggestions,
            session_id=session_id,
            model=input.model
        )
    except Exception as e:
        logging.error(f"L·ªói khi kh·ªõp CV {cv_id}: {str(e)}")
        return MatchResponse(
                name=cv_info.get("name", ""),
                email=cv_info.get("email", ""),
                phone=cv_info.get("phone", ""),
                cv_skills=skills,
                career_objective=aspirations,
                education=education,
                experience=experience,
                matched_jobs=[],
                suggestions=[{"skill_or_experience": "N/A", "suggestion": f"Failed to match jobs: {str(e)}"}],
                session_id=session_id,
                model=input.model
            )
    except json.JSONDecodeError as e:
        logging.error(f"JSON kh√¥ng h·ª£p l·ªá trong CV {cv_id if cv_id else 'ch∆∞a x√°c ƒë·ªãnh'}: {str(e)}")
        raise HTTPException(status_code=500, detail="D·ªØ li·ªáu JSON CV kh√¥ng h·ª£p l·ªá")
    except Exception as e:
        logging.error(f"L·ªói khi truy c·∫≠p CV {cv_id if cv_id else 'ch∆∞a x√°c ƒë·ªãnh'}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Kh√¥ng th·ªÉ truy c·∫≠p d·ªØ li·ªáu CV: {str(e)}")
    
@app.get("/list-cvs", response_model=List[DocumentInfo])
async def list_cvs(page: int = 1, page_size: int = 10):
    """Li·ªát k√™ t·∫•t c·∫£ CV trong cv_store v·ªõi ph√¢n trang."""
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT id, filename, cv_info_json, upload_timestamp FROM cv_store ORDER BY upload_timestamp DESC LIMIT ? OFFSET ?",
                (page_size, (page - 1) * page_size)
            )
            cvs = [
                {
                    "id": row["id"],
                    "filename": row["filename"],
                    "cv_info_json": row["cv_info_json"],
                    "upload_timestamp": row["upload_timestamp"]
                }
                for row in cursor.fetchall()
            ]
            logging.info(f"L·∫•y ƒë∆∞·ª£c {len(cvs)} CV")
            return [DocumentInfo(**cv) for cv in cvs]
    except Exception as e:
        logging.error(f"L·ªói khi li·ªát k√™ CV: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Kh√¥ng th·ªÉ li·ªát k√™ CV: {str(e)}")

@app.post("/delete-cv")
async def delete_cv(request: DeleteFileRequest):
    """X√≥a CV kh·ªèi cv_store v√† Chroma."""
    if not isinstance(request.file_id, int):
        raise HTTPException(status_code=400, detail="file_id must be an integer")
    try:
        deleted = delete_cv_record(request.file_id)
        if not deleted:
            raise HTTPException(status_code=404, detail=f"CV {request.file_id} kh√¥ng t√¨m th·∫•y")
        deleted_chroma = delete_cv_from_chroma(request.file_id)
        logging.info(f"ƒê√£ x√≥a CV {request.file_id} kh·ªèi cv_store v√† Chroma")
        return {"message": f"CV {request.file_id} ƒë√£ ƒë∆∞·ª£c x√≥a"}
    except Exception as e:
        logging.error(f"L·ªói khi x√≥a CV {request.file_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Kh√¥ng th·ªÉ x√≥a CV: {str(e)}")


# ===== NEW ENDPOINTS =====

@app.get("/cv/{cv_id}/insights", response_model=CVInsightsResponse)
async def get_cv_insights_endpoint(cv_id: int):
    """
    Ph√¢n t√≠ch CV chuy√™n s√¢u - ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng, ƒëi·ªÉm m·∫°nh/y·∫øu

    Kh√°c v·ªõi /upload-cv (ch·ªâ parse th√¥ng tin c∆° b·∫£n),
    endpoint n√†y ph√¢n t√≠ch v√† ƒë√°nh gi√° CV b·∫±ng AI.
    """
    try:
        # Ki·ªÉm tra CV t·ªìn t·∫°i
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT cv_info_json FROM cv_store WHERE id = ?", (cv_id,))
            row = cursor.fetchone()
            if not row:
                raise HTTPException(status_code=404, detail=f"CV {cv_id} kh√¥ng t√¨m th·∫•y")
            cv_info = json.loads(row["cv_info_json"])

        # Ki·ªÉm tra cache
        cached_insights = get_cv_insights(cv_id)
        if cached_insights:
            logging.info(f"‚úÖ L·∫•y insights t·ª´ cache cho CV {cv_id}")
            return CVInsightsResponse(
                cv_id=cv_id,
                quality_score=cached_insights['quality_score'],
                completeness={
                    "has_portfolio": False,
                    "has_certifications": False,
                    "has_projects": False,
                    "missing_sections": cached_insights['missing_sections']
                },
                market_fit={
                    "skill_match_rate": cached_insights['market_fit_score'],
                    "experience_level": "Junior",
                    "salary_range": "8-12 tri·ªáu",
                    "competitive_score": cached_insights['completeness_score'] * 10
                },
                strengths=cached_insights['strengths'],
                weaknesses=cached_insights['weaknesses'],
                last_analyzed=cached_insights['last_analyzed']
            )

        # Ph√¢n t√≠ch m·ªõi b·∫±ng AI
        logging.info(f"üîç B·∫Øt ƒë·∫ßu ph√¢n t√≠ch CV {cv_id}...")
        insights = await analyze_cv_insights(cv_info)

        # L∆∞u v√†o cache
        save_cv_insights(cv_id, insights)

        logging.info(f"‚úÖ Ph√¢n t√≠ch CV {cv_id} ho√†n t·∫•t")
        return CVInsightsResponse(
            cv_id=cv_id,
            quality_score=insights.get('quality_score', 5.0),
            completeness={
                "has_portfolio": insights.get('has_portfolio', False),
                "has_certifications": insights.get('has_certifications', False),
                "has_projects": insights.get('has_projects', False),
                "missing_sections": insights.get('missing_sections', [])
            },
            market_fit={
                "skill_match_rate": insights.get('market_fit_score', 0.5),
                "experience_level": insights.get('experience_level', 'Unknown'),
                "salary_range": insights.get('salary_range', 'N/A'),
                "competitive_score": insights.get('competitive_score', 5.0)
            },
            strengths=insights.get('strengths', []),
            weaknesses=insights.get('weaknesses', []),
            last_analyzed=datetime.now().isoformat()
        )

    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"‚ùå L·ªói ph√¢n t√≠ch CV {cv_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"L·ªói ph√¢n t√≠ch CV: {str(e)}")


@app.post("/cv/improve", response_model=CVImproveResponse)
async def improve_cv_endpoint(cv_id: int = Query(..., description="ID c·ªßa CV c·∫ßn c·∫£i thi·ªán")):
    """
    G·ª£i √Ω c·∫£i thi·ªán CV c·ª• th·ªÉ

    D·ª±a tr√™n k·∫øt qu·∫£ ph√¢n t√≠ch t·ª´ /cv/{cv_id}/insights,
    endpoint n√†y ƒë∆∞a ra c√°c g·ª£i √Ω h√†nh ƒë·ªông c·ª• th·ªÉ.
    """
    try:
        # L·∫•y CV info
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT cv_info_json FROM cv_store WHERE id = ?", (cv_id,))
            row = cursor.fetchone()
            if not row:
                raise HTTPException(status_code=404, detail=f"CV {cv_id} kh√¥ng t√¨m th·∫•y")
            cv_info = json.loads(row["cv_info_json"])

        # L·∫•y insights (ho·∫∑c ph√¢n t√≠ch m·ªõi)
        insights = get_cv_insights(cv_id)
        if not insights:
            logging.info(f"Ch∆∞a c√≥ insights, ph√¢n t√≠ch CV {cv_id} tr∆∞·ªõc...")
            insights_data = await analyze_cv_insights(cv_info)
            save_cv_insights(cv_id, insights_data)
            insights = insights_data

        # T·∫°o g·ª£i √Ω c·∫£i thi·ªán
        logging.info(f"üí° T·∫°o g·ª£i √Ω c·∫£i thi·ªán cho CV {cv_id}...")
        improvements = await generate_cv_improvements(cv_info, insights)

        improvement_suggestions = [
            ImprovementSuggestion(**imp) for imp in improvements
        ]

        logging.info(f"‚úÖ T·∫°o {len(improvement_suggestions)} g·ª£i √Ω c·∫£i thi·ªán")
        return CVImproveResponse(
            cv_id=cv_id,
            improvements=improvement_suggestions
        )

    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"‚ùå L·ªói t·∫°o g·ª£i √Ω c·∫£i thi·ªán CV {cv_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"L·ªói t·∫°o g·ª£i √Ω: {str(e)}")


@app.post("/jobs/search", response_model=JobSearchResponse)
async def search_jobs_endpoint(input: JobSearchInput):
    """
    T√¨m ki·∫øm jobs th√¥ng minh v·ªõi AI ranking

    Kh√°c v·ªõi /jobs (ch·ªâ list t·∫•t c·∫£),
    endpoint n√†y cho ph√©p search v·ªõi query, filters v√† AI ranking theo CV.
    """
    try:
        query = input.query
        filters = input.filters
        cv_id = input.cv_id
        limit = input.limit
        offset = input.offset

        logging.info(f"üîç T√¨m ki·∫øm jobs: query='{query}', filters={filters}, cv_id={cv_id}")

        # L·∫•y CV info n·∫øu c√≥ cv_id (ƒë·ªÉ AI ranking)
        cv_info = None
        if cv_id:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT cv_info_json FROM cv_store WHERE id = ?", (cv_id,))
                row = cursor.fetchone()
                if row:
                    cv_info = json.loads(row["cv_info_json"])

        # Build SQL query
        sql = "SELECT * FROM job_store WHERE 1=1"
        params = []

        # Text search
        if query:
            sql += " AND (job_title LIKE ? OR job_description LIKE ? OR skills LIKE ?)"
            search_term = f"%{query}%"
            params.extend([search_term, search_term, search_term])

        # Filters
        if filters.get('work_location'):
            locations = filters['work_location']
            placeholders = ','.join(['?' for _ in locations])
            sql += f" AND work_location IN ({placeholders})"
            params.extend(locations)

        if filters.get('work_type'):
            work_types = filters['work_type']
            placeholders = ','.join(['?' for _ in work_types])
            sql += f" AND work_type IN ({placeholders})"
            params.extend(work_types)

        if filters.get('experience'):
            sql += " AND experience = ?"
            params.append(filters['experience'])

        if filters.get('salary_min'):
            # Simple salary filter (can be improved)
            pass

        # Pagination
        sql += " LIMIT ? OFFSET ?"
        params.extend([limit, offset])

        # Execute query
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(sql, params)
            jobs = [dict(row) for row in cursor.fetchall()]

        # Count total
        count_sql = sql.split("LIMIT")[0]
        count_params = params[:-2]
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(f"SELECT COUNT(*) as total FROM ({count_sql})", count_params)
            total = cursor.fetchone()['total']

        # AI ranking n·∫øu c√≥ cv_id - SIMPLE MATCHING (kh√¥ng d√πng semantic search)
        results = []

        for job in jobs:
            job_skills = [s.strip() for s in job.get('skills', '').split(';') if s.strip()]

            match_score = None
            why_match = None

            if cv_info:
                cv_skills = cv_info.get('skills', [])
                matched_skills = set(cv_skills) & set(job_skills)
                match_score = len(matched_skills) / max(len(job_skills), 1) if job_skills else 0.0

                # Generate simple why_match
                if matched_skills:
                    why_match = f"Kh·ªõp {len(matched_skills)} k·ªπ nƒÉng: {', '.join(list(matched_skills)[:3])}"
                else:
                    why_match = "C√≥ th·ªÉ ph√π h·ª£p v·ªõi v·ªã tr√≠ n√†y"

            # Get company name from various possible fields
            company_name = job.get('name') or job.get('company_name') or job.get('company') or 'Unknown Company'

            results.append(JobSearchResult(
                job_id=job['id'],
                job_title=job['job_title'],
                company_name=company_name,
                match_score=match_score,
                salary=job.get('salary', 'N/A'),
                work_location=job.get('work_location', 'N/A'),
                work_type=job.get('work_type', 'N/A'),
                deadline=job.get('deadline', 'N/A'),
                why_match=why_match
            ))

        # Sort by match_score if available
        if cv_info:
            results.sort(key=lambda x: x.match_score or 0, reverse=True)

        logging.info(f"‚úÖ T√¨m ƒë∆∞·ª£c {total} jobs, tr·∫£ v·ªÅ {len(results)} jobs")
        return JobSearchResponse(
            total=total,
            jobs=results,
            limit=limit,
            offset=offset
        )

    except Exception as e:
        logging.error(f"‚ùå L·ªói t√¨m ki·∫øm jobs: {str(e)}")
        raise HTTPException(status_code=500, detail=f"L·ªói t√¨m ki·∫øm: {str(e)}")


@app.post("/apply", response_model=ApplicationResponse)
async def apply_job_endpoint(input: ApplyJobInput):
    """
    ·ª®ng tuy·ªÉn job - L∆∞u l·∫°i h√†nh ƒë·ªông ·ª©ng tuy·ªÉn

    Endpoint m·ªõi ƒë·ªÉ tracking vi·ªác ·ª©ng tuy·ªÉn c·ªßa user.
    """
    try:
        cv_id = input.cv_id
        job_id = input.job_id
        cover_letter = input.cover_letter
        status = input.status

        # Ki·ªÉm tra CV t·ªìn t·∫°i
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT id FROM cv_store WHERE id = ?", (cv_id,))
            if not cursor.fetchone():
                raise HTTPException(status_code=404, detail=f"CV {cv_id} kh√¥ng t√¨m th·∫•y")

        # Ki·ªÉm tra job t·ªìn t·∫°i
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT id FROM job_store WHERE id = ?", (job_id,))
            if not cursor.fetchone():
                raise HTTPException(status_code=404, detail=f"Job {job_id} kh√¥ng t√¨m th·∫•y")

        # Ki·ªÉm tra ƒë√£ apply ch∆∞a
        if check_application_exists(cv_id, job_id):
            raise HTTPException(status_code=400, detail=f"ƒê√£ ·ª©ng tuy·ªÉn job {job_id} r·ªìi")

        # L∆∞u application
        app_id = insert_application(cv_id, job_id, cover_letter, status)

        logging.info(f"‚úÖ CV {cv_id} ƒë√£ ·ª©ng tuy·ªÉn job {job_id}")
        return ApplicationResponse(
            application_id=app_id,
            cv_id=cv_id,
            job_id=job_id,
            status=status,
            applied_at=datetime.now().isoformat(),
            message=f"ƒê√£ ·ª©ng tuy·ªÉn job {job_id} th√†nh c√¥ng"
        )

    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"‚ùå L·ªói ·ª©ng tuy·ªÉn: {str(e)}")
        raise HTTPException(status_code=500, detail=f"L·ªói ·ª©ng tuy·ªÉn: {str(e)}")


@app.get("/applications/{cv_id}", response_model=ApplicationsResponse)
async def get_applications_endpoint(
    cv_id: int,
    status: Optional[str] = Query(None, description="L·ªçc theo status (applied, pending, accepted, rejected)")
):
    """
    L·∫•y l·ªãch s·ª≠ ·ª©ng tuy·ªÉn c·ªßa CV

    Endpoint m·ªõi ƒë·ªÉ xem c√°c job ƒë√£ apply.
    """
    try:
        # Ki·ªÉm tra CV t·ªìn t·∫°i
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT id FROM cv_store WHERE id = ?", (cv_id,))
            if not cursor.fetchone():
                raise HTTPException(status_code=404, detail=f"CV {cv_id} kh√¥ng t√¨m th·∫•y")

        # L·∫•y applications
        applications = get_applications_by_cv(cv_id, status)

        app_items = [
            ApplicationItem(
                id=app['id'],
                cv_id=app['cv_id'],
                job_id=app['job_id'],
                job_title=app['job_title'],
                company_url=app['company_url'],
                salary=app['salary'],
                work_location=app['work_location'],
                status=app['status'],
                applied_at=app['applied_at']
            )
            for app in applications
        ]

        logging.info(f"‚úÖ L·∫•y {len(app_items)} applications cho CV {cv_id}")
        return ApplicationsResponse(
            cv_id=cv_id,
            total=len(app_items),
            applications=app_items
        )

    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"‚ùå L·ªói l·∫•y applications: {str(e)}")
        raise HTTPException(status_code=500, detail=f"L·ªói l·∫•y applications: {str(e)}")


@app.get("/preview-doc/{file_id}")
async def preview_document_pdf(file_id: int):
    """
    Serve PDF file ƒë·ªÉ preview trong browser
    """
    try:
        from fastapi.responses import FileResponse, Response
        import os

        # L·∫•y th√¥ng tin CV
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT filename, file_data FROM cv_store WHERE id = ?", (file_id,))
            row = cursor.fetchone()
            if not row:
                raise HTTPException(status_code=404, detail=f"File {file_id} kh√¥ng t√¨m th·∫•y")

        # N·∫øu kh√¥ng c√≥ file_data (CV c≈©), tr·∫£ v·ªÅ placeholder
        if not row['file_data']:
            logging.warning(f"‚ö†Ô∏è CV {file_id} kh√¥ng c√≥ file_data. Tr·∫£ v·ªÅ placeholder.")
            return Response(
                content=f"<html><body><h3>CV Preview kh√¥ng kh·∫£ d·ª•ng</h3><p>File: {row['filename']}</p><p>Vui l√≤ng upload l·∫°i CV ƒë·ªÉ xem preview.</p></body></html>",
                media_type="text/html"
            )

        # T·∫°o th∆∞ m·ª•c temp n·∫øu ch∆∞a c√≥
        temp_dir = os.path.join(os.path.dirname(__file__), "..", "temp_pdfs")
        os.makedirs(temp_dir, exist_ok=True)

        # L∆∞u PDF v√†o temp file
        temp_file_path = os.path.join(temp_dir, f"cv_{file_id}.pdf")

        with open(temp_file_path, 'wb') as f:
            f.write(row['file_data'])

        logging.info(f"‚úÖ T·∫°o preview PDF cho file {file_id}")

        # Tr·∫£ v·ªÅ PDF file v·ªõi header inline ƒë·ªÉ hi·ªÉn th·ªã trong browser
        return FileResponse(
            temp_file_path,
            media_type='application/pdf',
            headers={
                "Content-Disposition": f"inline; filename={row['filename']}"
            }
        )

    except Exception as e:
        logging.error(f"‚ùå L·ªói preview PDF: {str(e)}")
        raise HTTPException(status_code=500, detail=f"L·ªói preview: {str(e)}")


@app.get("/preview-doc-info/{file_id}", response_model=DocumentPreviewResponse)
async def preview_document_info(file_id: int):
    """
    Xem tr∆∞·ªõc t√†i li·ªáu (CV) - Hi·ªÉn th·ªã summary, thumbnail

    Endpoint m·ªõi ƒë·ªÉ preview CV tr∆∞·ªõc khi submit ho·∫∑c xem l·∫°i.
    """
    try:
        # Ki·ªÉm tra cache
        cached_preview = get_document_preview(file_id)
        if cached_preview:
            logging.info(f"‚úÖ L·∫•y preview t·ª´ cache cho file {file_id}")

            # L·∫•y th√¥ng tin CV
            with get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT filename, cv_info_json FROM cv_store WHERE id = ?", (file_id,))
                row = cursor.fetchone()
                if not row:
                    raise HTTPException(status_code=404, detail=f"File {file_id} kh√¥ng t√¨m th·∫•y")

                cv_info = json.loads(row["cv_info_json"])

                return DocumentPreviewResponse(
                    file_id=file_id,
                    type=cached_preview['type'],
                    filename=row['filename'],
                    preview={
                        "title": f"CV - {cv_info.get('name', 'Unknown')}",
                        "summary": cached_preview['summary'],
                        "page_count": cached_preview['page_count'],
                        "file_size": f"{cached_preview['file_size'] / 1024:.1f} KB" if cached_preview['file_size'] else "N/A"
                    },
                    quick_info={
                        "name": cv_info.get('name', 'N/A'),
                        "email": cv_info.get('email', 'N/A'),
                        "phone": cv_info.get('phone', 'N/A'),
                        "top_skills": cv_info.get('skills', [])[:5]
                    }
                )

        # T·∫°o preview m·ªõi
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT filename, cv_info_json FROM cv_store WHERE id = ?", (file_id,))
            row = cursor.fetchone()
            if not row:
                raise HTTPException(status_code=404, detail=f"File {file_id} kh√¥ng t√¨m th·∫•y")

            cv_info = json.loads(row["cv_info_json"])

            # T·∫°o summary
            skills_str = ", ".join(cv_info.get('skills', [])[:5])
            summary = f"{cv_info.get('name', 'Unknown')} - {skills_str}"

            # L∆∞u preview
            preview_data = {
                "type": "cv",
                "summary": summary,
                "page_count": 1,  # Placeholder
                "file_size": 0  # Placeholder
            }
            save_document_preview(file_id, preview_data)

            logging.info(f"‚úÖ T·∫°o preview m·ªõi cho file {file_id}")
            return DocumentPreviewResponse(
                file_id=file_id,
                type="cv",
                filename=row['filename'],
                preview={
                    "title": f"CV - {cv_info.get('name', 'Unknown')}",
                    "summary": summary,
                    "page_count": 1,
                    "file_size": "N/A"
                },
                quick_info={
                    "name": cv_info.get('name', 'N/A'),
                    "email": cv_info.get('email', 'N/A'),
                    "phone": cv_info.get('phone', 'N/A'),
                    "top_skills": cv_info.get('skills', [])[:5]
                }
            )

    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"‚ùå L·ªói preview document: {str(e)}")
        raise HTTPException(status_code=500, detail=f"L·ªói preview: {str(e)}")


@app.post("/suggest-questions", response_model=SuggestQuestionsResponse)
async def suggest_questions_endpoint(input: SuggestQuestionsInput):
    """
    G·ª£i √Ω c√¢u h·ªèi d·ª±a tr√™n context

    Endpoint m·ªõi ƒë·ªÉ h∆∞·ªõng d·∫´n user h·ªèi AI nh·ªØng c√¢u h·ªèi ph√π h·ª£p.
    """
    try:
        context = input.context
        cv_id = input.cv_id
        job_id = input.job_id

        # L·∫•y CV info n·∫øu c√≥
        cv_info = None
        if cv_id:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT cv_info_json FROM cv_store WHERE id = ?", (cv_id,))
                row = cursor.fetchone()
                if row:
                    cv_info = json.loads(row["cv_info_json"])

        # L·∫•y job info n·∫øu c√≥
        job_info = None
        if job_id:
            with get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT job_title FROM job_store WHERE id = ?", (job_id,))
                row = cursor.fetchone()
                if row:
                    job_info = {"job_title": row["job_title"]}

        # T·∫°o g·ª£i √Ω c√¢u h·ªèi
        suggestions = generate_question_suggestions(context, cv_info, job_info)

        question_items = [
            QuestionSuggestion(**q) for q in suggestions
        ]

        logging.info(f"‚úÖ T·∫°o {len(question_items)} g·ª£i √Ω c√¢u h·ªèi cho context '{context}'")
        return SuggestQuestionsResponse(suggestions=question_items)

    except Exception as e:
        logging.error(f"‚ùå L·ªói t·∫°o g·ª£i √Ω c√¢u h·ªèi: {str(e)}")
        raise HTTPException(status_code=500, detail=f"L·ªói t·∫°o g·ª£i √Ω: {str(e)}")


# ===== FRONTEND ENDPOINTS =====

@app.get("/cvs")
async def get_all_cvs_simple():
    """
    L·∫•y t·∫•t c·∫£ CVs v·ªõi th√¥ng tin ƒë√£ parse (cho frontend dashboard)

    Kh√°c v·ªõi /list-cvs (c√≥ ph√¢n trang), endpoint n√†y tr·∫£ v·ªÅ t·∫•t c·∫£ CVs
    v·ªõi cv_info ƒë√£ ƒë∆∞·ª£c parse th√†nh object (kh√¥ng ph·∫£i JSON string)
    """
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT id, filename, cv_info_json, upload_timestamp FROM cv_store ORDER BY upload_timestamp DESC")
            rows = cursor.fetchall()

            cvs = []
            for row in rows:
                cv_info = json.loads(row["cv_info_json"]) if row["cv_info_json"] else {}
                cvs.append({
                    "id": row["id"],
                    "filename": row["filename"],
                    "cv_info": cv_info,  # Already parsed object
                    "upload_timestamp": row["upload_timestamp"]
                })

            logging.info(f"‚úÖ L·∫•y {len(cvs)} CVs cho frontend")
            return cvs

    except Exception as e:
        logging.error(f"‚ùå L·ªói l·∫•y CVs: {str(e)}")
        raise HTTPException(status_code=500, detail=f"L·ªói l·∫•y CVs: {str(e)}")


@app.get("/jobs")
async def get_all_jobs_simple(limit: int = 100, offset: int = 0):
    """
    L·∫•y t·∫•t c·∫£ jobs (cho frontend dashboard v√† jobs listing)

    Kh√°c v·ªõi /jobs/search (c·∫ßn cv_id v√† AI ranking),
    endpoint n√†y ch·ªâ ƒë∆°n gi·∫£n list jobs v·ªõi pagination

    Parameters:
    - limit: S·ªë l∆∞·ª£ng jobs t·ªëi ƒëa (default: 100)
    - offset: V·ªã tr√≠ b·∫Øt ƒë·∫ßu (default: 0)

    Returns:
    - jobs: List of job objects
    - total: T·ªïng s·ªë jobs trong database
    - limit: Limit ƒë∆∞·ª£c s·ª≠ d·ª•ng
    - offset: Offset ƒë∆∞·ª£c s·ª≠ d·ª•ng
    """
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()

            # Get total count
            cursor.execute("SELECT COUNT(*) as total FROM job_store")
            total = cursor.fetchone()["total"]

            # Get jobs with pagination
            cursor.execute(f"SELECT * FROM job_store LIMIT ? OFFSET ?", (limit, offset))
            jobs = [dict(row) for row in cursor.fetchall()]

            logging.info(f"‚úÖ L·∫•y {len(jobs)} jobs (total: {total}, limit: {limit}, offset: {offset})")

            return {
                "jobs": jobs,
                "total": total,
                "limit": limit,
                "offset": offset
            }

    except Exception as e:
        logging.error(f"‚ùå L·ªói l·∫•y jobs: {str(e)}")
        raise HTTPException(status_code=500, detail=f"L·ªói l·∫•y jobs: {str(e)}")


@app.get("/jobs/analytics")
async def get_jobs_analytics():
    """
    Ph√¢n t√≠ch xu h∆∞·ªõng vi·ªác l√†m - Dashboard Analytics

    Returns:
    - top_job_titles: Top 10 v·ªã tr√≠ tuy·ªÉn d·ª•ng nhi·ªÅu nh·∫•t
    - top_companies: Top 10 c√¥ng ty tuy·ªÉn d·ª•ng nhi·ªÅu nh·∫•t
    - salary_distribution: Ph√¢n b·ªë m·ª©c l∆∞∆°ng
    - location_distribution: Ph√¢n b·ªë ƒë·ªãa ƒëi·ªÉm l√†m vi·ªác
    - job_type_distribution: Ph√¢n b·ªë lo·∫°i h√¨nh c√¥ng vi·ªác
    - experience_distribution: Ph√¢n b·ªë y√™u c·∫ßu kinh nghi·ªám
    - top_skills: Top 20 k·ªπ nƒÉng ƒë∆∞·ª£c y√™u c·∫ßu nhi·ªÅu nh·∫•t
    - deadline_stats: Th·ªëng k√™ deadline (s·∫Øp h·∫øt h·∫°n, c√≤n l√¢u)
    """
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()

            # 1. Top 10 Job Titles
            cursor.execute("""
                SELECT job_title, COUNT(*) as count
                FROM job_store
                WHERE job_title IS NOT NULL AND job_title != ''
                GROUP BY job_title
                ORDER BY count DESC
                LIMIT 10
            """)
            top_job_titles = [{"title": row["job_title"], "count": row["count"]} for row in cursor.fetchall()]

            # 2. Top 10 Companies
            cursor.execute("""
                SELECT name, COUNT(*) as count
                FROM job_store
                WHERE name IS NOT NULL AND name != ''
                GROUP BY name
                ORDER BY count DESC
                LIMIT 10
            """)
            top_companies = [{"company": row["name"], "count": row["count"]} for row in cursor.fetchall()]

            # 3. Salary Distribution (ph√¢n lo·∫°i theo range)
            cursor.execute("""
                SELECT salary, COUNT(*) as count
                FROM job_store
                WHERE salary IS NOT NULL AND salary != '' AND salary != 'Th·ªèa thu·∫≠n'
                GROUP BY salary
                ORDER BY count DESC
                LIMIT 15
            """)
            salary_distribution = [{"salary": row["salary"], "count": row["count"]} for row in cursor.fetchall()]

            # 4. Location Distribution
            cursor.execute("""
                SELECT work_location, COUNT(*) as count
                FROM job_store
                WHERE work_location IS NOT NULL AND work_location != ''
                GROUP BY work_location
                ORDER BY count DESC
                LIMIT 10
            """)
            location_distribution = [{"location": row["work_location"], "count": row["count"]} for row in cursor.fetchall()]

            # 5. Job Type Distribution (work_type column)
            cursor.execute("""
                SELECT work_type, COUNT(*) as count
                FROM job_store
                WHERE work_type IS NOT NULL AND work_type != ''
                GROUP BY work_type
                ORDER BY count DESC
            """)
            job_type_distribution = [{"type": row["work_type"], "count": row["count"]} for row in cursor.fetchall()]

            # 6. Experience Distribution
            cursor.execute("""
                SELECT experience, COUNT(*) as count
                FROM job_store
                WHERE experience IS NOT NULL AND experience != ''
                GROUP BY experience
                ORDER BY count DESC
            """)
            experience_distribution = [{"experience": row["experience"], "count": row["count"]} for row in cursor.fetchall()]

            # 7. Top Skills (parse t·ª´ skills JSON array)
            cursor.execute("SELECT skills FROM job_store WHERE skills IS NOT NULL AND skills != ''")
            skills_counter = {}
            for row in cursor.fetchall():
                try:
                    skills_list = json.loads(row["skills"]) if isinstance(row["skills"], str) else row["skills"]
                    if isinstance(skills_list, list):
                        for skill in skills_list:
                            if skill and isinstance(skill, str):
                                skill = skill.strip()
                                skills_counter[skill] = skills_counter.get(skill, 0) + 1
                except:
                    pass

            top_skills = [{"skill": skill, "count": count} for skill, count in sorted(skills_counter.items(), key=lambda x: x[1], reverse=True)[:20]]

            # 8. Deadline Stats (s·∫Øp h·∫øt h·∫°n trong 7 ng√†y, 30 ng√†y)
            from datetime import datetime, timedelta
            today = datetime.now().date()
            deadline_7_days = (today + timedelta(days=7)).isoformat()
            deadline_30_days = (today + timedelta(days=30)).isoformat()

            cursor.execute("""
                SELECT
                    COUNT(CASE WHEN deadline <= ? THEN 1 END) as expiring_7_days,
                    COUNT(CASE WHEN deadline <= ? THEN 1 END) as expiring_30_days,
                    COUNT(*) as total
                FROM job_store
                WHERE deadline IS NOT NULL AND deadline != ''
            """, (deadline_7_days, deadline_30_days))
            deadline_row = cursor.fetchone()
            deadline_stats = {
                "expiring_7_days": deadline_row["expiring_7_days"] or 0,
                "expiring_30_days": deadline_row["expiring_30_days"] or 0,
                "total_with_deadline": deadline_row["total"] or 0
            }

            # 9. Total Stats
            cursor.execute("SELECT COUNT(*) as total FROM job_store")
            total_jobs = cursor.fetchone()["total"]

            logging.info(f"‚úÖ Ph√¢n t√≠ch {total_jobs} jobs th√†nh c√¥ng")

            return {
                "total_jobs": total_jobs,
                "top_job_titles": top_job_titles,
                "top_companies": top_companies,
                "salary_distribution": salary_distribution,
                "location_distribution": location_distribution,
                "job_type_distribution": job_type_distribution,
                "experience_distribution": experience_distribution,
                "top_skills": top_skills,
                "deadline_stats": deadline_stats
            }

    except Exception as e:
        logging.error(f"‚ùå L·ªói ph√¢n t√≠ch jobs: {str(e)}")
        raise HTTPException(status_code=500, detail=f"L·ªói ph√¢n t√≠ch: {str(e)}")

@app.post("/jobs/analytics/insights")
async def generate_chart_insights(request: Dict[str, Any]):
    """
    Generate AI insights for dashboard charts using LLM

    Request body:
    {
        "chart_type": "top_jobs" | "top_companies" | "location" | "job_type" | "experience" | "salary",
        "data": [...chart data...]
    }

    Returns:
    {
        "analysis": "AI-generated analysis text in Vietnamese"
    }
    """
    try:
        chart_type = request.get("chart_type")
        data = request.get("data", [])

        if not chart_type or not data:
            return {"analysis": "Thi·∫øu th√¥ng tin bi·ªÉu ƒë·ªì ƒë·ªÉ ph√¢n t√≠ch."}

        # Import LLM from ai_analysis
        from ai_analysis import llm

        # Create prompt based on chart type
        if chart_type == "top_jobs":
            data_str = "\n".join([f"- {item.get('title', 'N/A')}: {item.get('count', 0)} vi·ªác l√†m" for item in data[:10]])
            prompt = f"""B·∫°n l√† chuy√™n gia ph√¢n t√≠ch th·ªã tr∆∞·ªùng vi·ªác l√†m. Ph√¢n t√≠ch bi·ªÉu ƒë·ªì "Top 10 V·ªã Tr√≠ Tuy·ªÉn D·ª•ng Nhi·ªÅu Nh·∫•t" d·ª±a tr√™n d·ªØ li·ªáu sau:

{data_str}

Cung c·∫•p ph√¢n t√≠ch ng·∫Øn g·ªçn d∆∞·ªõi d·∫°ng vƒÉn b·∫£n li√™n t·ª•c (3-4 c√¢u), kh√¥ng s·ª≠ d·ª•ng ti√™u ƒë·ªÅ ch√†o h·ªèi, kh√¥ng ƒë√°nh s·ªë ho·∫∑c bullet points, ch·ªâ t·∫≠p trung v√†o n·ªôi dung ch√≠nh:
- Xu h∆∞·ªõng tuy·ªÉn d·ª•ng ch√≠nh
- Ng√†nh ngh·ªÅ hot nh·∫•t
- C∆° h·ªôi cho ·ª©ng vi√™n
- L·ªùi khuy√™n

Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, chuy√™n nghi·ªáp, s√∫c t√≠ch."""

        elif chart_type == "top_companies":
            data_str = "\n".join([f"- {item.get('company', 'N/A')}: {item.get('count', 0)} vi·ªác l√†m" for item in data[:10]])
            prompt = f"""B·∫°n l√† chuy√™n gia ph√¢n t√≠ch th·ªã tr∆∞·ªùng vi·ªác l√†m. Ph√¢n t√≠ch bi·ªÉu ƒë·ªì "Top 10 C√¥ng Ty Tuy·ªÉn D·ª•ng Nhi·ªÅu Nh·∫•t" d·ª±a tr√™n d·ªØ li·ªáu sau:

{data_str}

Cung c·∫•p ph√¢n t√≠ch ng·∫Øn g·ªçn d∆∞·ªõi d·∫°ng vƒÉn b·∫£n li√™n t·ª•c (3-4 c√¢u), kh√¥ng s·ª≠ d·ª•ng ti√™u ƒë·ªÅ ch√†o h·ªèi, kh√¥ng ƒë√°nh s·ªë ho·∫∑c bullet points, ch·ªâ t·∫≠p trung v√†o n·ªôi dung ch√≠nh:
- C√¥ng ty ƒëang m·ªü r·ªông
- Lƒ©nh v·ª±c kinh doanh
- C∆° h·ªôi ph√°t tri·ªÉn
- L·ªùi khuy√™n cho ·ª©ng vi√™n

Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, chuy√™n nghi·ªáp, s√∫c t√≠ch."""

        elif chart_type == "location":
            data_str = "\n".join([f"- {item.get('location', 'N/A')}: {item.get('count', 0)} vi·ªác l√†m" for item in data[:10]])
            prompt = f"""B·∫°n l√† chuy√™n gia ph√¢n t√≠ch th·ªã tr∆∞·ªùng vi·ªác l√†m. Ph√¢n t√≠ch bi·ªÉu ƒë·ªì "Ph√¢n B·ªë ƒê·ªãa ƒêi·ªÉm L√†m Vi·ªác" d·ª±a tr√™n d·ªØ li·ªáu sau:

{data_str}

Cung c·∫•p ph√¢n t√≠ch ng·∫Øn g·ªçn d∆∞·ªõi d·∫°ng vƒÉn b·∫£n li√™n t·ª•c (3-4 c√¢u), kh√¥ng s·ª≠ d·ª•ng ti√™u ƒë·ªÅ ch√†o h·ªèi, kh√¥ng ƒë√°nh s·ªë ho·∫∑c bullet points, ch·ªâ t·∫≠p trung v√†o n·ªôi dung ch√≠nh:
- Th√†nh ph·ªë c√≥ nhi·ªÅu c∆° h·ªôi nh·∫•t
- Xu h∆∞·ªõng ph√¢n b·ªë ƒë·ªãa l√Ω
- So s√°nh c√°c th√†nh ph·ªë
- L·ªùi khuy√™n theo ƒë·ªãa ƒëi·ªÉm

Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, chuy√™n nghi·ªáp, s√∫c t√≠ch."""

        elif chart_type == "job_type":
            data_str = "\n".join([f"- {item.get('type', 'N/A')}: {item.get('count', 0)} vi·ªác l√†m" for item in data])
            prompt = f"""B·∫°n l√† chuy√™n gia ph√¢n t√≠ch th·ªã tr∆∞·ªùng vi·ªác l√†m. Ph√¢n t√≠ch bi·ªÉu ƒë·ªì "Ph√¢n B·ªë Lo·∫°i H√¨nh C√¥ng Vi·ªác" d·ª±a tr√™n d·ªØ li·ªáu sau:

{data_str}

Cung c·∫•p ph√¢n t√≠ch ng·∫Øn g·ªçn d∆∞·ªõi d·∫°ng vƒÉn b·∫£n li√™n t·ª•c (3-4 c√¢u), kh√¥ng s·ª≠ d·ª•ng ti√™u ƒë·ªÅ ch√†o h·ªèi, kh√¥ng ƒë√°nh s·ªë ho·∫∑c bullet points, ch·ªâ t·∫≠p trung v√†o n·ªôi dung ch√≠nh:
- Lo·∫°i h√¨nh ph·ªï bi·∫øn nh·∫•t
- Xu h∆∞·ªõng remote/hybrid/onsite
- C∆° h·ªôi freelance/part-time
- L·ªùi khuy√™n theo lo·∫°i h√¨nh

Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, chuy√™n nghi·ªáp, s√∫c t√≠ch."""

        elif chart_type == "experience":
            data_str = "\n".join([f"- {item.get('experience', 'N/A')}: {item.get('count', 0)} vi·ªác l√†m" for item in data[:10]])
            prompt = f"""B·∫°n l√† chuy√™n gia ph√¢n t√≠ch th·ªã tr∆∞·ªùng vi·ªác l√†m. Ph√¢n t√≠ch bi·ªÉu ƒë·ªì "Ph√¢n B·ªë Y√™u C·∫ßu Kinh Nghi·ªám" d·ª±a tr√™n d·ªØ li·ªáu sau:

{data_str}

Cung c·∫•p ph√¢n t√≠ch ng·∫Øn g·ªçn d∆∞·ªõi d·∫°ng vƒÉn b·∫£n li√™n t·ª•c (3-4 c√¢u), kh√¥ng s·ª≠ d·ª•ng ti√™u ƒë·ªÅ ch√†o h·ªèi, kh√¥ng ƒë√°nh s·ªë ho·∫∑c bullet points, ch·ªâ t·∫≠p trung v√†o n·ªôi dung ch√≠nh:
- M·ª©c kinh nghi·ªám ƒë∆∞·ª£c y√™u c·∫ßu nhi·ªÅu
- C∆° h·ªôi cho fresher vs experienced
- Xu h∆∞·ªõng tuy·ªÉn d·ª•ng
- L·ªùi khuy√™n cho t·ª´ng nh√≥m

Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, chuy√™n nghi·ªáp, s√∫c t√≠ch."""

        elif chart_type == "salary":
            data_str = "\n".join([f"- {item.get('salary', 'N/A')}: {item.get('count', 0)} vi·ªác l√†m" for item in data[:10]])
            prompt = f"""B·∫°n l√† chuy√™n gia ph√¢n t√≠ch th·ªã tr∆∞·ªùng vi·ªác l√†m. Ph√¢n t√≠ch bi·ªÉu ƒë·ªì "Ph√¢n B·ªë M·ª©c L∆∞∆°ng" d·ª±a tr√™n d·ªØ li·ªáu sau:

{data_str}

Cung c·∫•p ph√¢n t√≠ch ng·∫Øn g·ªçn d∆∞·ªõi d·∫°ng vƒÉn b·∫£n li√™n t·ª•c (3-4 c√¢u), kh√¥ng s·ª≠ d·ª•ng ti√™u ƒë·ªÅ ch√†o h·ªèi, kh√¥ng ƒë√°nh s·ªë ho·∫∑c bullet points, ch·ªâ t·∫≠p trung v√†o n·ªôi dung ch√≠nh:
- M·ª©c l∆∞∆°ng ph·ªï bi·∫øn
- Xu h∆∞·ªõng l∆∞∆°ng theo ng√†nh
- So s√°nh th·ªã tr∆∞·ªùng
- L·ªùi khuy√™n th∆∞∆°ng l∆∞·ª£ng l∆∞∆°ng

Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, chuy√™n nghi·ªáp, s√∫c t√≠ch."""

        else:
            return {"analysis": "Lo·∫°i bi·ªÉu ƒë·ªì kh√¥ng h·ª£p l·ªá."}

        # Call LLM with API key rotation
        logging.info(f"Generating analysis for chart type: {chart_type}")
        from ai_analysis import get_llm
        llm_instance = get_llm()
        response = await llm_instance.ainvoke(prompt)
        analysis = response.content.strip()

        logging.info(f"Generated analysis: {analysis[:100]}...")
        return {"analysis": analysis}

    except Exception as e:
        logging.error(f"L·ªói generate chart insights: {e}")
        return {"analysis": "Kh√¥ng th·ªÉ t·∫°o ph√¢n t√≠ch. Vui l√≤ng th·ª≠ l·∫°i sau."}